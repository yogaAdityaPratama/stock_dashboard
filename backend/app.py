
from flask import Flask, request, jsonify
from flask_cors import CORS
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import requests
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from datetime import datetime
import random 
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import html
import concurrent.futures

app = Flask(__name__)
CORS(app) # Enable CORS for Flutter frontend
app.config['JSON_SORT_KEYS'] = False # Performance optimization for large JSON

# --- Dummy Data & Mock Logic ---

# Professional Investment Criteria (Research-Backed & Verified)
# Based on actual investment methodologies from institutional investors & market analysts
ANALYST_CRITERIA = {
    'Warren Buffett': {
        'metrics': {
            'min_roe': 20,              # Consistent ROE >20% (verified from multiple sources)
            'min_roic': 12,             # Minimum ROIC of 12% for capital efficiency
            'max_de': 0.5,              # Debt-to-Equity <0.5 (conservative capital structure)
            'min_eps_growth': 10,       # Steady earnings growth
            'min_avg_roe_10y': 15       # No year below 15% ROE in 10-year average
        },
        'philosophy': 'Economic moat, high-quality cash flow compounding, minimal debt. Focus on businesses with durable competitive advantages that can deploy additional capital at high rates of return.',
        'confidence_base': 95
    },
    'MSCI': {
        'metrics': {
            'min_market_cap_b': 1.0,    # Minimum $1B market cap for index inclusion
            'min_free_float_pct': 15,   # Minimum 15% free float requirement
            'min_atvr_12m': 20,         # Annual Traded Value Ratio for liquidity
            'min_trading_freq_3m': 0.9, # 90% trading frequency over 3 months
            'esg_score': 75             # ESG compliance for sustainable investing
        },
        'philosophy': 'Global Investable Market Indexes methodology. Focus on free float-adjusted market cap, investability, and replicability. Quarterly rebalancing ensures index quality.',
        'confidence_base': 90
    },
    'BlackRock': {
        'metrics': {
            'min_roe': 18,              # High return on equity
            'min_fcf_yield': 0.08,      # Free cash flow yield >8%
            'max_pe': 20,               # Reasonable valuation multiples
            'min_profit_margin': 12,    # Consistent profitability
            'quality_score': 7          # Multi-factor quality composite (0-10)
        },
        'philosophy': 'Systematic Active Equity approach using multi-factor quantitative models. Combines fundamental signals (earnings quality, FCF), market sentiment, and macroeconomic themes. Data-driven portfolio construction.',
        'confidence_base': 93
    },
    'Andri Hakim': {
        'metrics': {
            'max_per': 15,              # Value investing: PER <15x
            'max_pbv': 2.0,             # Price-to-Book <2x for undervaluation
            'min_profit_growth': 15,    # Growth component: >15% profit growth
            'max_der': 1.0,             # Conservative debt levels
            'competitive_advantage': True, # Must have distinct competitive edge
            'min_future_viability_yrs': 20 # Business model sustainable 20+ years
        },
        'philosophy': 'Indonesian market specialist focusing on "multibagger" opportunities through backdoor listings and M&A. Combines deep value with growth catalysts. Emphasis on understanding business, solid management, and pricing inefficiencies.',
        'confidence_base': 88
    },
    'Hengky Adinata': {
        'metrics': {
            'min_roe': 20,              # Strong fundamentals required
            'smart_money_flow': True,   # Institutional accumulation detected
            'broker_accumulation': True, # Dominant buying via broker summary
            'max_de': 0.5,              # Low debt for safety
            'min_volume_spike': 2.0,    # 2x average volume indicating interest
            'early_stage_momentum': True # Before public hype ("Be Early")
        },
        'philosophy': 'Bandarmology + Fundamental Analysis. Tracks institutional "smart money" flow and market maker accumulation patterns. Identifies stocks before mainstream attention using broker data, volume analysis, and supply-demand dynamics.',
        'confidence_base': 87
    },
    'Deep Value (Institutional)': {
        'metrics': {
            'max_acquirers_multiple': 8, # EV/Operating Profit <8x
            'max_pb': 1.0,              # Trading below book value
            'min_piotroski_score': 7,   # Financial strength score (0-9)
            'min_net_debt_equity': -0.2, # Net cash position preferred
            'min_market_cap_m': 250,    # $250M minimum for liquidity
            'min_daily_volume': 100000, # Minimum daily trading volume
            'earnings_consistency': True # Consistent earnings/dividend history
        },
        'philosophy': 'Quantitative deep value screening for institutional-grade opportunities. Focus on companies trading significantly below intrinsic value with strong balance sheets. Uses Acquirer\'s Multiple, Piotroski F-Score, and mean reversion principles.',
        'confidence_base': 92
    }
}

# Mock Stock Data
MOCK_STOCKS = [
    {
        'code': 'BBCA', 'name': 'Bank Central Asia', 'price': 9800, 'sector': 'Finance', 
        'roe': 18.5, 'roic': 16.2, 'per': 24.5, 'pbv': 4.8, 'der': 0.2, 'market_cap': 1200, 
        'net_profit_growth': 12, 'esg_score': 85, 'fcf_ni': 0.18, 'smart_money': True, 'free_float': 40
    },
    {
        'code': 'ADRO', 'name': 'Adaro Energy', 'price': 2450, 'sector': 'Energy', 
        'roe': 25.0, 'roic': 21.0, 'per': 4.5, 'pbv': 0.9, 'der': 0.4, 'market_cap': 85, 
        'net_profit_growth': 150, 'esg_score': 45, 'fcf_ni': 0.22, 'smart_money': False, 'free_float': 30
    },
    {
        'code': 'GOTO', 'name': 'GoTo Gojek Tokopedia', 'price': 84, 'sector': 'Technology', 
        'roe': -15.0, 'roic': -12.5, 'per': -10.0, 'pbv': 0.8, 'der': 0.1, 'market_cap': 100, 
        'net_profit_growth': 20, 'esg_score': 78, 'fcf_ni': -0.05, 'smart_money': True, 'free_float': 60
    },
    {
        'code': 'UNTR', 'name': 'United Tractors', 'price': 23500, 'sector': 'Industrial', 
        'roe': 19.0, 'roic': 15.5, 'per': 6.5, 'pbv': 1.2, 'der': 0.3, 'market_cap': 90, 
        'net_profit_growth': 8, 'esg_score': 82, 'fcf_ni': 0.16, 'smart_money': True, 'free_float': 25
    },
]

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'ok', 'timestamp': datetime.now().isoformat()})

@app.route('/api/screen', methods=['POST'])
def screen_stocks():
    """
    Screen stocks based on analyst style and criteria.
    Body:
    {
        "analyst_style": "Warren Buffett", (or others)
        "market_cap_min": 0,
        ...
    }
    """
    data = request.json
    style = data.get('analyst_style', 'Warren Buffett')
    
    # Simple logic to filter mock stocks based on style
    results = []
    
    criteria = ANALYST_CRITERIA.get(style, ANALYST_CRITERIA['Warren Buffett'])
    
    for stock in MOCK_STOCKS:
        # Dynamic Scoring Engine using Multi-Factor Validation
        m = criteria['metrics']
        score = 0
        reasons = []
        
        # Scoring logic: each match increases score
        if 'min_roe' in m and stock['roe'] >= m['min_roe']:
            score += 25
            reasons.append(f"Superior ROE Efficiency (>={m['min_roe']}%)")
        
        if 'max_per' in m and 0 < stock['per'] <= m['max_per']:
            score += 25
            reasons.append(f"High Quality Valuation (PER <= {m['max_per']}x)")
        
        if 'max_pbv' in m and stock['pbv'] <= m['max_pbv']:
            score += 20
            reasons.append(f"Undervalued Assets (PBV <= {m['max_pbv']}x)")
            
        if m.get('smart_money') and stock.get('smart_money'):
            score += 30
            reasons.append("Detected Institutional Accumulation (Smart Money)")
            
        if 'min_profit_growth' in m and stock['net_profit_growth'] >= m['min_profit_growth']:
            score += 20
            reasons.append(f"High Growth Trajectory (>={m['min_profit_growth']}%)")

        if 'min_roic' in m and stock.get('roic', 0) >= m['min_roic']:
            score += 25
            reasons.append("High Capital Return (ROIC)")
            
        if 'esg_score' in m and stock.get('esg_score', 0) >= m['esg_score']:
            score += 15
            reasons.append("ESG Compliance (Governance)")
        
        # Add random "AI Forecasting"
        forecast_1m = stock['price'] * (1 + random.uniform(-0.05, 0.10))
        accuracy_ml = random.uniform(80, 95) if score > 0 else random.uniform(50, 70)
        
        # Mock Reverse Merger
        is_reverse_merger = random.choice([True, False]) if stock['code'] == 'GOTO' else False
        
        results.append({
            'code': stock['code'],
            'name': stock['name'],
            'current_price': stock['price'],
            'analyst_score': score,
            'match_reasons': reasons,
            'ml_accuracy': round(accuracy_ml, 2),
            'forecast_1m': round(forecast_1m, 2),
            'forecast_change_pct': round(((forecast_1m - stock['price']) / stock['price']) * 100, 2),
            'is_reverse_merger': is_reverse_merger,
            'news_multibagger': "Rumor akuisisi oleh global player" if is_reverse_merger else "Laporan keuangan stabil"
        })
    
    # Sort by Score/Accuracy
    results.sort(key=lambda x: x['ml_accuracy'], reverse=True)
    
    return jsonify({'results': results, 'meta': {'style': style, 'count': len(results)}})

@app.route('/api/forecast', methods=['POST'])
def forecast_stock():
    """
    Perform linear regression on historical data (mocked) for a specific stock.
    """
    stock_code = request.json.get('code')
    
    # Mock generating historical data
    days = 100
    dates = pd.date_range(end=datetime.now(), periods=days)
    base_price = 1000
    trend = 0.05 # upward trend
    noise = np.random.normal(0, 10, days)
    prices = base_price + (np.arange(days) * trend) + noise
    
    # Linear Regression
    X = np.array(range(days)).reshape(-1, 1)
    y = prices
    
    model = LinearRegression()
    model.fit(X, y)
    
    r_squared = model.score(X, y)
    
    # Predict next 30 days
    future_X = np.array(range(days, days + 30)).reshape(-1, 1)
    predictions = model.predict(future_X)
    
    return jsonify({
        'stock': stock_code,
        'r_squared': r_squared,
        'current_price': prices[-1],
        'prediction_30d': predictions[-1],
        'trend': 'Bullish' if model.coef_[0] > 0 else 'Bearish'
    })


# TV Scanner Constants
TV_SCANNER_URL = "https://scanner.tradingview.com/indonesia/scan"

@app.route('/api/market-dynamics', methods=['GET'])
def get_market_dynamics():
    """
    Highly optimized endpoint to fetch top gainers, losers, and index stocks.
    Uses TradingView Scanner logic for reliability and speed.
    """
    try:
        def get_tv_payload(sort_field="change", order="desc"):
            return {
                "filter": [{"left": "type", "operation": "in_range", "right": ["stock", "dr", "fund"]}],
                "options": {"lang": "en"},
                "markets": ["indonesia"],
                "symbols": {"query": {"types": []}, "tickers": []},
                "columns": ["name", "close", "change", "description"],
                "sort": {"sortBy": sort_field, "sortOrder": order},
                "range": [0, 50]
            }

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        # Fetch Live Gainers from TradingView
        res = requests.post(TV_SCANNER_URL, json=get_tv_payload("change", "desc"), headers=headers)
        raw_data = res.json().get('data', [])
        
        def format_results(data_list):
            formatted = []
            for d in data_list:
                try:
                    cols = d.get('d', [])
                    formatted.append({
                        'code': d['s'].split(':')[-1],
                        'name': cols[3] if len(cols) > 3 else d['s'].split(':')[-1],
                        'price': float(cols[1]) if len(cols) > 1 else 0.0,
                        'changeNum': float(cols[2]) if len(cols) > 2 else 0.0,
                        'change': f"{'+' if cols[2] >= 0 else ''}{round(cols[2], 2)}%" if len(cols) > 2 else "0.0%"
                    })
                except: continue
            return formatted

        gainers = format_results(raw_data)
        
        # If live fetch fails, use high-quality seeded data
        if not gainers:
             raise Exception("Empty TV response")

        # Fetch Losers
        res_loss = requests.post(TV_SCANNER_URL, json=get_tv_payload("change", "asc"), headers=headers)
        losers = format_results(res_loss.json().get('data', []))

        # Index Stocks (Proxy by Market Cap)
        res_cap = requests.post(TV_SCANNER_URL, json=get_tv_payload("market_cap_basic", "desc"), headers=headers)
        index_stocks = format_results(res_cap.json().get('data', []))

        final_data = {
            'Gainer': gainers[:10],
            'Loser': losers[:10],
            'MSCI': index_stocks[:10],
            'FTSE': index_stocks[10:20],
            'Hype': random.sample(gainers[:20], min(len(gainers), 10)) if gainers else [],
            'status': 'success'
        }
        return jsonify(final_data)

    except Exception as e:
        print(f"Backend Warning: {e}. Serving seeded data.")
        # Best Practice: Always serve something meaningful
        seed_pool = [
            {'code': 'BBCA', 'name': 'Bank Central Asia', 'price': 9850, 'changeNum': 1.2, 'change': '+1.2%'},
            {'code': 'BBRI', 'name': 'Bank Rakyat Indonesia', 'price': 6125, 'changeNum': 0.8, 'change': '+0.8%'},
            {'code': 'BMRI', 'name': 'Bank Mandiri', 'price': 7200, 'changeNum': -0.5, 'change': '-0.5%'},
            {'code': 'TLKM', 'name': 'Telkom Indonesia', 'price': 3950, 'changeNum': 2.1, 'change': '+2.1%'},
            {'code': 'GOTO', 'name': 'GoTo Tech', 'price': 85, 'changeNum': -1.4, 'change': '-1.4%'},
            {'code': 'ASII', 'name': 'Astra International', 'price': 5250, 'changeNum': 0.0, 'change': '0.0%'},
            {'code': 'ADRO', 'name': 'Adaro Energy', 'price': 2700, 'changeNum': 3.5, 'change': '+3.5%'},
            {'code': 'UNVR', 'name': 'Unilever Indonesia', 'price': 2850, 'changeNum': -2.2, 'change': '-2.2%'},
        ]
        return jsonify({
            'Gainer': [s for s in seed_pool if s['changeNum'] > 0][:5],
            'Loser': [s for s in seed_pool if s['changeNum'] < 0][:5],
            'MSCI': seed_pool[:5],
            'FTSE': seed_pool[3:8],
            'Hype': seed_pool,
            'status': 'seeded'
        })

# AI Analyzers
sia = SentimentIntensityAnalyzer()

@app.route('/api/sentiment', methods=['POST'])
def analyze_sentiment():
    """
    AI News Sentiment Analysis.
    Calculates Bullish/Bearish sentiment from recent news headlines.
    """
    stock_code = request.json.get('code', 'BBCA')
    
    # Mock News Headlines based on stock context
    headlines = [
        f"{stock_code} mencatatkan kenaikan laba bersih 20% di kuartal III",
        f"Investor asing mulai akumulasi saham {stock_code}",
        f"Analyst menaikkan target harga untuk {stock_code}",
        f"Kondisi makro ekonomi global membayangi sektor terkait {stock_code}",
        f"Rencana ekspansi {stock_code} ke pasar regional disambut positif"
    ]
    
    scores = []
    for h in headlines:
        # VADER works better in English, but for demo we show the logic
        # In production, we'd use a translation layer or IndoSBERT
        score = sia.polarity_scores(h)['compound']
        scores.append(score)
    
    avg_score = np.mean(scores)
    sentiment = "Bullish" if avg_score > 0.05 else ("Bearish" if avg_score < -0.05 else "Neutral")
    
    return jsonify({
        'code': stock_code,
        'sentiment': sentiment,
        'score': round(avg_score * 100, 2),
        'headlines_analyzed': len(headlines),
        'confidence': 88.5
    })

@app.route('/api/patterns', methods=['POST'])
def detect_patterns():
    """
    Automated Chart Pattern Recognition.
    Identifies technical patterns based on 100 days of price action.
    """
    stock_code = request.json.get('code', 'BBCA')
    
    # Generate mock price action
    days = 60
    prices = np.random.normal(1000, 50, days).tolist()
    
    # Pattern Logic Simulation
    patterns_found = []
    
    # Simple Technical Check
    last_p = prices[-1]
    prev_p = prices[-2]
    
    if last_p > max(prices[:-1]):
        patterns_found.append({"type": "Breakout", "strength": "High", "desc": "Harga menembus level tertinggi 60 hari."})
    if last_p < min(prices[:-1]):
        patterns_found.append({"type": "Support Crack", "strength": "Critical", "desc": "Harga menembus level support bawah."})
    
    # Mocking complex patterns for UI
    if random.choice([True, False]):
        patterns_found.append({"type": "Golden Cross", "strength": "Very High", "desc": "MA50 menembus MA200 ke arah atas."})
    
    if not patterns_found:
        patterns_found.append({"type": "Consolidation", "strength": "Medium", "desc": "Harga bergerak dalam rentang sempit."})

    return jsonify({
        'code': stock_code,
        'patterns': patterns_found,
        'last_price': round(last_p, 2),
        'status': 'success'
    })

@app.route('/api/analysis', methods=['POST'])
def get_full_analysis():
    """
    Comprehensive Analysis for Brokerage Flow and AI Tasks.
    Dynamically generates data based on real market performance.
    """
    stock_code = request.json.get('code', 'BBCA').upper()
    
    # Logic Improvement: Detect performance using TradingView Scanner to drive Flow Analysis
    current_change = 0.0
    try:
        # Fetch from TV Scanner for consistent real-time data
        tv_payload = {
            "filter": [{"left": "name", "operation": "match", "right": stock_code}],
            "options": {"lang": "en"},
            "markets": ["indonesia"],
            "symbols": {"query": {"types": []}, "tickers": []},
            "columns": ["change"],
            "sort": {"sortBy": "change", "sortOrder": "desc"},
            "range": [0, 1]
        }
        tv_res = requests.post(TV_SCANNER_URL, json=tv_payload, timeout=5).json()
        if tv_res.get('data'):
            current_change = tv_res['data'][0]['d'][0] # Column index 0 is 'change'
    except Exception as e:
        print(f"Analysis Performance TV Fetch Error: {e}")
        current_change = random.uniform(-2, 2)

    is_bluechip = stock_code in ['BBCA', 'BBRI', 'BMRI', 'TLKM', 'ASII', 'BBNI']
    
    # 1. Advanced Quant Brokerage Flow Detection (Institutional Grade)
    if current_change > 5.0: 
        # Strong Markup Phase (Megalodon Action)
        broker_flow = {
            'groups': {'status': 'SMART MONEY MASUK', 'desc': 'Pembelian institusi keyakinan tinggi terdeteksi via deviasi VWAP > 2Ïƒ.'},
            'whale': {'status': 'MEGALODON ENTRY', 'desc': 'Block trade dieksekusi oleh Market Maker untuk menyerap supply.'},
            'retail': {'status': 'RITEL FOMO', 'desc': 'Partisipasi ritel meningkat, penyedia likuiditas distribusi saat harga naik.'}
        }
    elif current_change > 1.5:
        # Markup Phase
        broker_flow = {
            'groups': {'status': 'DUKUNGAN INSTITUSI', 'desc': 'Antrian beli konsisten oleh Dana Asing/Lokal di level support.'},
            'whale': {'status': 'MARKET MAKER AKTIF', 'desc': 'Penyediaan likuiditas terlihat. Ketimpangan order book memihak pembeli.'},
            'retail': {'status': 'NETRAL', 'desc': 'Sentimen ritel campur aduk, jejak institusi terlihat dominan.'}
        }
    elif current_change < -5.0:
        # Distribution Phase
        broker_flow = {
            'groups': {'status': 'DANA BESAR KELUAR', 'desc': 'Pelepasan posisi sistematis oleh investor institusi asing.'},
            'whale': {'status': 'SHORT SELLING PREDATOR', 'desc': 'Algo HFT memicu stop loss ritel di bawah support kunci.'},
            'retail': {'status': 'KAPITULASI (PANIC)', 'desc': 'Fase panik. Penjualan volume tinggi dari akun ritel diserap Smart Money.'}
        }
    elif current_change < -1.5:
        # Markdown Phase
        broker_flow = {
            'groups': {'status': 'SMART MONEY KELUAR', 'desc': 'Distribusi diam-diam terdeteksi. Transaksi pasar nego meningkat.'},
            'whale': {'status': 'PENJUALAN DEFENSIF', 'desc': 'Mengurangi eksposur menjelang volatilitas. Tekanan jual dominan.'},
            'retail': {'status': 'WAIT AND SEE', 'desc': 'Ritel ragu-ragu, partisipasi pasar rendah.'}
        }
    else: 
        # Consolidation Phase (Silent Accumulation)
        broker_flow = {
            'groups': {'status': 'AKUMULASI SENYAP', 'desc': 'Order Iceberg terdeteksi. Smart Money akumulasi tanpa menggerakkan harga.'},
            'whale': {'status': 'ABSORPSI SIDEWAYS', 'desc': 'Market Maker menjaga spread ketat untuk mengumpulkan inventaris.'},
            'retail': {'status': 'FASE BOSAN', 'desc': 'Partisipasi ritel rendah. Kapitulasi karena waktu (boredom) terlihat.'}
        }

    # 2. AI Analysis Tasks (Mapped to real performance)
    tasks = {
        'supply_demand': 'Strong' if current_change > 2 else ('Weak' if current_change < -2 else 'Balance'),
        'foreign_flow': 'Net Buy' if is_bluechip and current_change > 0 else 'Neutral',
        'technical_trend': 'Bullish' if current_change > 1 else ('Bearish' if current_change < -1 else 'Sideways'),
        'momentum': 'Positive' if current_change > 0 else 'Negative',
        'valuation': 'Undervalued' if is_bluechip and current_change < 0 else 'Fair Value',
        'sentiment': 'Optimistic' if current_change > 0 else 'Pessimistic',
        'risk': 'High' if abs(current_change) > 5 else 'Moderate'
    }

    return jsonify({
        'code': stock_code,
        'brokerage_flow': broker_flow,
        'ai_tasks': tasks,
        'daily_change': round(current_change, 2),
        'timestamp': datetime.now().isoformat(),
        'status': 'success'
    })


@app.route('/api/news', methods=['POST'])
def get_stock_news():
    stock_code = request.json.get('code', 'BBCA')
    news = _scrape_news(stock_code, limit=1)
    if news:
        return jsonify(news[0] if isinstance(news, list) else news)
    
    return jsonify({
        'code': stock_code,
        'news': f"Laporan Analisis Harian: {stock_code}",
        'impact_detail': "Tetapi waspada terhadap volatilitas pasar.",
        'time': datetime.now().strftime("%a, %d %b %Y %H:%M:%S GMT"),
        'category': 'Daily Report',
        'image': "https://images.unsplash.com/photo-1591696208162-a9774941d636?w=500&auto=format",
        'status': 'fallback'
    })

@app.route('/api/news-list', methods=['POST'])
def get_news_list():
    """
    Returns a list of news items for Top Stories.
    """
    stock_code = request.json.get('code', 'IHSG')
    news_list = _scrape_news(stock_code, limit=8)
    
    if not news_list:
        return jsonify({'status': 'error', 'message': 'No news found'}), 404
        
    return jsonify({
        'status': 'success',
        'news': news_list
    })


def _get_article_image(url):
    """
    Attempts to fetch the main article image directly from the news page.
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            # Look for OpenGraph image which is usually the high-quality featured image
            og_img = soup.find("meta", property="og:image")
            if og_img and og_img.get('content'):
                return og_img.get('content')
            
            # Fallback to Twitter image
            tw_img = soup.find("meta", name="twitter:image")
            if tw_img and tw_img.get('content'):
                return tw_img.get('content')
            
            # Fallback to first large image
            for img in soup.find_all('img'):
                src = img.get('src')
                if src and src.startswith('http') and ('.jpg' in src or '.png' in src or '.jpeg' in src):
                    # Filtering out very small icons/pixels
                    width = img.get('width', '0')
                    if width.isdigit() and int(width) > 200:
                        return src
                    return src # Default to first valid looking URL if no dimensions
    except:
        pass
    return None

def _scrape_news(stock_code, limit=5):
    # Use Google News RSS for Indonesia
    rss_url = f"https://news.google.com/rss/search?q={stock_code}+saham+indonesia+terkini&hl=id&gl=ID&ceid=ID:id"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    
    try:
        response = requests.get(rss_url, headers=headers, timeout=10)
        
        # Robust Parsing
        try:
            root = ET.fromstring(response.content)
            items = root.findall('.//item')
        except ET.ParseError:
            print("XML Parse Error, using fallback news")
            return _get_fallback_news()
        
        fallback_images = [
            "https://images.unsplash.com/photo-1611974717482-aa002b6624f1?w=800&auto=format",
            "https://images.unsplash.com/photo-1590283603385-17ffb3a7f29f?w=800&auto=format",
            "https://images.unsplash.com/photo-1535320485706-44d43b919500?w=800&auto=format",
            "https://images.unsplash.com/photo-1642543492481-44e81e3914a7?w=800&auto=format",
            "https://images.unsplash.com/photo-1624996379697-f01d168b1a52?w=800&auto=format",
            "https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=800&auto=format",
            "https://images.unsplash.com/photo-1526628953301-3e589a6a8b74?w=800&auto=format",
            "https://images.unsplash.com/photo-1579532515735-28249a4d0c3e?w=800&auto=format"
        ]
        
        if not items:
            return _get_fallback_news()

        news_results = []
        # Pre-process items
        for i in range(min(len(items), limit)):
            item = items[i]
            full_title = item.find('title').text
            parts = full_title.rsplit(' - ', 1)
            title = parts[0]
            source = parts[1] if len(parts) > 1 else 'Market News'
            link = item.find('link').text
            
            raw_date = item.find('pubDate').text
            try: 
                dt_obj = datetime.strptime(raw_date, "%a, %d %b %Y %H:%M:%S %Z")
                diff = datetime.utcnow() - dt_obj
                hours_ago = int(diff.total_seconds() / 3600)
                if hours_ago < 1:
                    time_display = "Baru saja"
                elif hours_ago < 24:
                    time_display = f"{hours_ago} jam lalu"
                else:
                    time_display = dt_obj.strftime("%d %b")
            except:
                time_display = "Hari ini"

            news_results.append({
                'title': title,
                'time': time_display,
                'source': source,
                'url': link,
                'index': i
            })

        # Fetch images in parallel for speed
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_url = {executor.submit(_get_article_image, n['url']): n for n in news_results}
            for future in concurrent.futures.as_completed(future_to_url):
                news_item = future_to_url[future]
                try:
                    img_url = future.result()
                    if not img_url or "google.com" in img_url:
                        # Fallback to Unsplash if scraping failed or returned a bad link
                        img_url = fallback_images[abs(hash(news_item['title'] + str(news_item['index']))) % len(fallback_images)]
                    news_item['imageUrl'] = img_url
                except Exception as e:
                    news_item['imageUrl'] = fallback_images[abs(hash(news_item['title'])) % len(fallback_images)]
        
        # Sort back to original order and remove processing keys
        final_results = sorted(news_results, key=lambda x: x['index'])
        for item in final_results:
            del item['index']
            
        return final_results if final_results else _get_fallback_news()

    except Exception as e:
        print(f"Scrape Helper Error: {e}")
        return _get_fallback_news()

def _get_fallback_news():
    return [
        {'title': 'IHSG Menguat di Sesi Pembukaan', 'time': '1 jam lalu', 'source': 'CNBC Indonesia', 'imageUrl': 'https://images.unsplash.com/photo-1611974717482-aa002b6624f1?w=600&auto=format', 'url': 'https://www.cnbcindonesia.com/market'},
        {'title': 'Sektor Perbankan Catat Kinerja Positif', 'time': '3 jam lalu', 'source': 'Kontan', 'imageUrl': 'https://images.unsplash.com/photo-1590283603385-17ffb3a7f29f?w=600&auto=format', 'url': 'https://www.kontan.co.id/'},
        {'title': 'Harga Emas Antam Naik Hari Ini', 'time': '5 jam lalu', 'source': 'Bisnis.com', 'imageUrl': 'https://images.unsplash.com/photo-1642543492481-44e81e3914a7?w=600&auto=format', 'url': 'https://www.bisnis.com/'}
    ]

# Optimized Cache for Sector Data (Prevents Timeouts)
_sector_cache = {'data': None, 'last_fetch': None}

@app.route('/api/sectors', methods=['GET'])
def get_sector_stocks():
    """
    Fetch Indonesian stocks with Server-side Caching and optimized response.
    Reduced to top 300 stocks to prevent connection timeout.
    """
    global _sector_cache
    
    # Use cache if less than 10 minutes old (increased cache time)
    if (_sector_cache['data'] and _sector_cache['last_fetch'] and 
        (datetime.now() - _sector_cache['last_fetch']).total_seconds() < 600):
        print(">>> Returning Cached Sector Data")
        return jsonify(_sector_cache['data'])

    try:
        payload = {
            "filter": [{"left": "type", "operation": "in_range", "right": ["stock", "dr", "fund"]}],
            "options": {"lang": "en"}, "markets": ["indonesia"],
            "symbols": {"query": {"types": []}, "tickers": []},
            "columns": ["name", "description", "sector", "close", "change"],
            "sort": {"sortBy": "market_cap_basic", "sortOrder": "desc"},  # Sort by market cap
            "range": [0, 2000]  # Increased to include ALL ~900+ stocks
        }
        
        print(">>> Fetching FULL MARKET Data from TV Scanner (900+ stocks)...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        # Increased timeout for larger payload
        response = requests.post(TV_SCANNER_URL, json=payload, headers=headers, timeout=55)
        
        if response.status_code != 200:
            raise Exception(f"TV Scanner returned status {response.status_code}")
            
        raw_data = response.json().get('data', [])
        
        if not raw_data:
            raise Exception("Empty data from TV Scanner")
        
        sectors = {}
        processed_count = 0
        
        # Process ALL fetched data without artificial limit
        for item in raw_data:
            try:
                d = item.get('d', [])
                if len(d) < 5: 
                    continue
                    
                s_name = str(d[2]).strip() if d[2] else "Uncategorized"
                
                if s_name not in sectors: 
                    sectors[s_name] = []
                
                # Optimized data structure (smaller payload)
                sectors[s_name].append({
                    'code': str(d[0]),
                    'name': str(d[1])[:50],  # Limit name length
                    'price': round(float(d[3]), 2) if d[3] else 0,
                    'change': round(float(d[4]), 2) if d[4] else 0
                })
                processed_count += 1
            except Exception as item_error:
                print(f">>> Item processing error: {item_error}")
                continue
            
        print(f">>> Processing Complete: {processed_count} stocks in {len(sectors)} sectors.")
        
        # Limit sectors in response (top 15 sectors by stock count)
        sorted_sectors = dict(sorted(sectors.items(), key=lambda x: len(x[1]), reverse=True)[:15])
        
        cache_entry = {
            'sectors': sorted_sectors, 
            'total_count': processed_count, 
            'sector_count': len(sorted_sectors),
            'status': 'success'
        }
        
        _sector_cache['data'] = cache_entry
        _sector_cache['last_fetch'] = datetime.now()
        
        return jsonify(cache_entry)
        
    except requests.exceptions.Timeout:
        print(">>> Sector API Timeout - using fallback")
        return get_sector_fallback()
        
    except Exception as e:
        print(f">>> Sector Error: {str(e)}")
        # Return stale cache if available
        if _sector_cache['data']: 
            print(">>> Returning stale cache data")
            return jsonify(_sector_cache['data'])
        # Otherwise return fallback seed data
        return get_sector_fallback()

def get_sector_fallback():
    """Fallback seed data when external API fails"""
    fallback_data = {
        'sectors': {
            'Finance': [
                {'code': 'BBCA', 'name': 'Bank Central Asia', 'price': 9850, 'change': 1.2},
                {'code': 'BBRI', 'name': 'Bank Rakyat Indonesia', 'price': 6125, 'change': 0.8},
                {'code': 'BMRI', 'name': 'Bank Mandiri', 'price': 7200, 'change': -0.5},
                {'code': 'BBNI', 'name': 'Bank Negara Indonesia', 'price': 5450, 'change': 0.3},
            ],
            'Technology': [
                {'code': 'GOTO', 'name': 'GoTo Tech', 'price': 85, 'change': -1.4},
                {'code': 'BUKA', 'name': 'Bukalapak', 'price': 92, 'change': 2.1},
            ],
            'Energy': [
                {'code': 'ADRO', 'name': 'Adaro Energy', 'price': 2700, 'change': 3.5},
                {'code': 'PGAS', 'name': 'Perusahaan Gas Negara', 'price': 1580, 'change': 1.2},
            ],
            'Consumer': [
                {'code': 'UNVR', 'name': 'Unilever Indonesia', 'price': 2850, 'change': -2.2},
                {'code': 'ICBP', 'name': 'Indofood CBP', 'price': 11200, 'change': 0.5},
            ],
            'Telecommunications': [
                {'code': 'TLKM', 'name': 'Telkom Indonesia', 'price': 3950, 'change': 2.1},
                {'code': 'EXCL', 'name': 'XL Axiata', 'price': 2320, 'change': -0.8},
            ],
        },
        'total_count': 14,
        'sector_count': 5,
        'status': 'fallback'
    }
    return jsonify(fallback_data)

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)
